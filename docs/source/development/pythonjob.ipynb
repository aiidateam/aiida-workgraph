{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "22d177dc-6cfb-4de2-9509-f1eb45e10cf2",
   "metadata": {},
   "source": [
    "# `PythonJob` to run Python function on a remote computer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58696c91",
   "metadata": {},
   "source": [
    "`PythonJob` node is a built-in node which allow user to run Python function on a remote computer.\n",
    "\n",
    "\n",
    "## File Handling\n",
    "\n",
    "### Remote Folder\n",
    "List the file in the remote folder:\n",
    "```console\n",
    "$ ls\n",
    "aiida.out        inputs.pickle   _scheduler-stderr.txt  script.py\n",
    "_aiidasubmit.sh  results.pickle  _scheduler-stdout.txt\n",
    "```\n",
    "\n",
    "Each node creates a `script.py` file on the remote computer, which includes:\n",
    "- The function definition.\n",
    "- Loading inputs from `inputs.pickle`.\n",
    "- Running the function with the loaded inputs.\n",
    "- Saving the results into `results.pickle`.\n",
    "\n",
    "\n",
    "### About the data\n",
    "For a `CalcJob`, the input data needs to be an AiiDA data node; however, we don't require the user to install AiiDA or the same Python environment on a remote computer. This means we should pass normal Python data as arguments when running the Python function on the remote computer. The `WorkGraphEngine` will handle this data transformation when preparing and launching the `CalcJob`.\n",
    "\n",
    "All AiiDA data that will be passed to the function should have a `value` attribute, which corresponds to its raw Python data. The `GeneralData`, `Int`, `Float`, `Str`, `Bool` fulfill this requirement, while `List`, `Dict` and `StructureData` are not.\n",
    "\n",
    "### Inputs and Outputs:\n",
    "Inputs for each node are pickled into the `inputs.pickle` file.\n",
    "Outputs from each node are pickled into the `results.pickle` file.\n",
    "\n",
    "### Parent Folder\n",
    "The parent_folder parameter allows a node to access the output files of a parent node. This feature is particularly useful when you want to reuse data generated by a previous computation in subsequent computations. In the provided example, the multiply node uses the result.txt file created by the add node.\n",
    "\n",
    "\n",
    "## Example\n",
    "\n",
    "```python\n",
    "from aiida_workgraph import WorkGraph, task\n",
    "from aiida import orm, load_profile\n",
    "\n",
    "load_profile()\n",
    "\n",
    "# define add node\n",
    "@task()\n",
    "def add(x, y):\n",
    "    return x + y\n",
    "\n",
    "# define multiply node\n",
    "@task()\n",
    "def multiply(x, y):\n",
    "    return x*y\n",
    "\n",
    "wg = WorkGraph(\"first_workflow\")\n",
    "wg.tasks.new(add, name=\"add\", run_remotely=True)\n",
    "wg.tasks.new(multiply, name=\"multiply\", x=wg.nodes[\"add\"].outputs[0], run_remotely=True)\n",
    "\n",
    "\n",
    "#------------------------- Submit the calculation -------------------\n",
    "wg.submit(inputs = {\"add\": {\"x\": 2, \"y\": 3, \"computer\": \"localhost\"},\n",
    "                    \"multiply\": {\"y\": 4, \"computer\": \"localhost\"}},\n",
    "          wait=True)\n",
    "print(\"\\nResult of multiply is {} \\n\\n\".format(wg.nodes[\"multiply\"].outputs['result'].value))\n",
    "```\n",
    "\n",
    "\n",
    "### Using `parent_folder_name` for Data Continuity\n",
    "\n",
    "AiiDA runs each job in a separate folder. If one calculation requires data from previous calculations to be accessible in the current job's working directory. This has been managed with the `parent_folder` input, which specifies a source for copying necessary data. The new `parent_folder_name` input streamlines this process by allowing users to define a subfolder within the working directory to organize these files effectively.\n",
    "\n",
    "#### Example Usage: NSCF Calculation\n",
    "In the context of an NSCF calculation, where data dependency exists on outputs from a SCF calculation, the workflow can be configured as follows:\n",
    "\n",
    "```python\n",
    "nscf_node = wg.tasks.new(\n",
    "    pw_calculator,\n",
    "    name=\"nscf\",\n",
    "    parent_folder=scf_node.outputs[\"remote_folder\"],\n",
    "    parent_output_folder=\"out\",\n",
    "    parent_folder_name=\"out\",\n",
    "    run_remotely=True,\n",
    ")\n",
    "```\n",
    "This setup will copy all content of the `out` folder from the SCF calculation's remote folder into an `out` folder within the working directory of the NSCF job.\n",
    "\n",
    "### Handling Multiple Data Sources with `copy_files`\n",
    "The traditional `parent_folder` method is limited when calculations require inputs from multiple remote directories. For instance, Bader charge analysis with Quantum ESPRESSO may need both valence and all-electron density data from different calculations.\n",
    "\n",
    "The new `copy_files` input allows for flexible linkage to multiple remote folders. It facilitates copying necessary files from diverse sources into a single job's directory under dynamically generated subfolder names based on node and socket names.\n",
    "\n",
    "#### Example Usage: Bader Charge Analysis\n",
    "For a Bader analysis requiring different charge density files:\n",
    "\n",
    "```python\n",
    "bader_node = wg.tasks.new(\n",
    "    bader_calculator,\n",
    "    name=\"bader\",\n",
    "    command=bader_command,\n",
    "    charge_density_folder=\"pp_valence_remote_folder\",\n",
    "    reference_charge_density_folder=\"pp_all_remote_folder\",\n",
    "    run_remotely=True,\n",
    ")\n",
    "wg.links.new(pp_valence.outputs[\"remote_folder\"], bader_node.inputs[\"copy_files\"])\n",
    "wg.links.new(pp_all.outputs[\"remote_folder\"], bader_node.inputs[\"copy_files\"])\n",
    "```\n",
    "\n",
    "The `bader_calculator` node function using specified charge density data:\n",
    "\n",
    "```python\n",
    "@task()\n",
    "def bader_calculator(\n",
    "    command: str = \"pw.x\",\n",
    "    charge_density_folder: str = \"./\",\n",
    "    charge_density_filename: str = \"charge_density.cube\",\n",
    "    reference_charge_density_folder: str = \"./\",\n",
    "    reference_charge_density_filename: str = \"charge_density.cube\",\n",
    "):\n",
    "    \"\"\"Run Bader charge analysis.\"\"\"\n",
    "    command_str = f\"{command} {charge_density_folder}/{charge_density_filename}\"\n",
    "    if reference_charge_density_filename:\n",
    "        command_str += f\" -ref {reference_charge_density_folder}/{reference_charge_density_filename}\"\n",
    "    os.system(command_str)\n",
    "\n",
    "    with open(\"ACF.dat\", \"r\") as f:\n",
    "        lines = f.readlines()\n",
    "        charges = [float(line.split()[4]) for line in lines[2:-4]]\n",
    "\n",
    "    return charges\n",
    "```\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.4 ('scinode')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "2f450c1ff08798c4974437dd057310afef0de414c25d1fd960ad375311c3f6ff"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
